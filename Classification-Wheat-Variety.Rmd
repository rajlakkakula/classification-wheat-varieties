---
title: 'Classification: Predicting Wheat Variety Using Kernel Geometrical Attributes'
author: "Prithviraj Lakkakula"
date: "03/18/2022"
output:
  pdf_document: 
    toc: yes
    highlight: zenburn
  html_document: 
    highlight: zenburn
    toc: yes
    theme: readable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(kernlab)
```

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

```{r include=FALSE}
#include=FALSE #Won't output this chunk of r code and results in final output
#echo=FALSE #prevents code, but not the results from appearing in the finished file
#warning = FALSE #wont display warning messages in the finished file
#message = FALSE # prevents warnings that are generated by code from apparing in the finshed file
#fig.cap = "..." #adds a caption to graphical results.

```

## Goal

The project's goal is to accurately predict the wheat variety (`Kama`, `Rosa`, `Canadian`) using the seven attributes corresponding to each of wheat variety.

## Data

In this project, I classify wheat variety based on the wheat kernel's geometrical properties. There are three varieties of wheat (`Kama`, `Rosa`, and `Canadian`), which is my class variable. Each variety has 70 observations accounting for a total of 210 observations. The features (X) are seven attributes, including area, perimeter, compactness, length of the kernel, width of the kernel, asymmetry coefficient, and length of kernel groove. Data are collected from UC Irvine Machine Learning Repository at <https://archive-beta.ics.uci.edu/ml/datasets/seeds>.

## Data Preprocessing

```{r, message = FALSE}
library(dplyr)
wht_data <- read.csv("wheat_var_data.csv")
glimpse(wht_data)
```

```{r}
summary(wht_data)
```

By inspecting mean and median of all seven attributes, one can conclude that there are no outliers/anomalies. Also, we need to convert the `wheat_variety` variable into categorical or qualitative or class variable instead of an integer.

```{r}
library(dplyr)
wht_data$wheat_variety <- as.factor(wht_data$wheat_variety)
wht_data <- wht_data %>%
  mutate(wheat_var = 
           ifelse(wheat_variety == "1", "Kama",      
                  ifelse(wheat_variety == "2", "Rosa", "Canadian"))) %>%
  select(-wheat_variety)
str(wht_data)
```

## Exploratory Data Analysis

Let us now look at the relationships of the three wheat varieties with each of the seven features.

```{r}
library(dplyr)
wht_data %>%
  group_by(wheat_var) %>%
  summarise_all(mean)
```

On average, `Rosa` wheat variety seem to have higher length, width, area, perimeter and compactness, followed by `Kama` variety. However, `Canadian` variety has the highest average asymmetry coefficient compared with other wheat varieties.


<!--
### Summary Statistics
```{r}
#library(vtable)
#st(wht_data)
```
-->

```{r fig.cap="Density plot of kernel length of three wheat varieties", message=FALSE}
library(ggplot2)
library(geomtextpath)

ggplot(wht_data, aes(x = length_kernel, colour = wheat_var, label = wheat_var)) +
  geom_textdensity(size = 6, fontface = 2, hjust = 0.2, vjust = 0.3) +
  theme(legend.position = "none") + theme_bw()
```

```{r fig.cap="Density plot of asymmetry coefficient of three wheat varieties"}
library(ggplot2)
library(geomtextpath)
ggplot(wht_data, aes(x = asymmetry_coef, colour = wheat_var, 
                     label = wheat_var)) +
       theme(legend.position = "none") +
       geom_textdensity(size = 6, fontface = 2, spacing = 50,
                   vjust = -0.2, hjust = "ymax") + ylim(c(0, 0.4)) + theme_minimal()
```

```{r, fig.cap= 'Trend Lines through scatter plot of length and width of wheat varieties'}
ggplot(wht_data, aes(x = length_kernel, y = width_kernel, 
                     color = wheat_var)) +
  geom_point(alpha = 0.3) + theme(legend.position = "bottom") +
  geom_labelsmooth(aes(label = wheat_var), text_smoothing = 30, 
                   fill = "#F6F6FF",
                method = "loess", formula = y ~ x,
                size = 4, linewidth = 1, boxlinewidth = 0.3) +
  scale_colour_manual(values = c("forestgreen", "deepskyblue4", "tomato4")) +
   theme_bw()
```

### Correlation Pairs

Now, let us look at the range of all the variables except the response variable.

```{r}
wht_data %>%
  select(-wheat_var) %>%
  summarise_all(range)
```

## Standardization of the features

Since some of the variables are in different range than the others. Let us do Z-score normalization or standardization the `scale()` function in R. When applying the decision trees (random forests and gradient boosting) and KNN machine leanrning algorithms, we may need not scale.

```{r warning=FALSE}
library(dplyr)
##Z-score normalization
wht_data_scaled <- wht_data %>% mutate_each_(list(~scale(.) %>% as.vector),
vars = c("area","perimeter", "compactness", 
         "length_kernel", "width_kernel", 
         "asymmetry_coef", "length_kernel_groove"))
head(wht_data_scaled)
```

### Near-zero variance features

```{r message=FALSE}
library(caret)
near_0_var <- nearZeroVar(wht_data, names = TRUE)
print(near_0_var)
```

It seems like there are no zero variance features, which is good. Therefore, we can use all the features to predict the the class of wheat variety.

### Checking for class imbalance

We already know that there are equal observations for each of the wheat variety in our dataset. That is, each variety has 70 observations for a total of 210 observations. Therefore, our data set do not suffer with class imbalance

```{r}
table(wht_data$wheat_var)
```

## Ensemble Models

### Splitting the data

```{r}
library(caret)

set.seed(4321)
wht_data_scaled$wheat_var <- as.factor(wht_data_scaled$wheat_var)
in_train <- createDataPartition(y = wht_data_scaled$wheat_var, 
                                p = 0.80, list = FALSE)

training <- wht_data_scaled[in_train,]
testing <- wht_data_scaled[-in_train,]

table(training$wheat_var)
table(testing$wheat_var)
head(training)
str(training)
```

### Classficiation: Random Forest

```{r message=FALSE}
### load the randomForest package
library(randomForest)

### train the random forest model: model_rf
model_rf <- randomForest(formula = wheat_var ~.,
                         data = training,
                         ntree = 500)

### print the rf model
print(model_rf)

### variable importance plots
varImpPlot(model_rf)
print(model_rf$importance)
```

### Classification : Gradient Boosting Model

```{r message=FALSE, warning=FALSE}
### load the gradient boosting model package
library(gbm)

### train the gradient boosting model: model_gbm
model_gbm <- gbm(formula = wheat_var ~.,
                         data = training,
                         n.trees = 500)

### print the gbm model
print(model_gbm)

### summarize gbm's variable importance plots
summary(model_gbm)
```

### Evaluating both Random Forest and Gradient Boosting Algorithms

```{r message=FALSE, warning=FALSE}
library(Metrics)

preds_rf <- predict(model_rf, newdata = testing)
preds_gbm <- predict(model_gbm, n.trees = 500, newdata = testing, type = "response")
## compute confusion matrix



classes <- colnames(preds_gbm)[apply(preds_gbm, 1, which.max)]
result_gbm <- data.frame(testing$wheat_var, classes)

#print(result_gbm)
(cm_rf <- confusionMatrix(preds_rf, testing$wheat_var))
(cm_gbm <- confusionMatrix(as.factor(classes), testing$wheat_var))

```

## Conclusions
The ensemble models suggest that there is an accuracy of about 95% using Random Forest and 93% using GBM in predicting the correct wheat variety using a set of features. In the UC Irvine's data repository, it was indicated that there was some critical features that they could not provide due to proprietary issues associated with those data. Therefore, given those additional features, there is a scope for improving accuracy rate. Overall, the classification results show that accuracy of predicting the correct wheat variety is high. 

<!--
### Training Ensemble Models
```{r}
library(caret)
library(caretEnsemble)
## Let us create a 5-fold cross valiadtion training control object
train_control <- trainControl(method = "cv",
                              number = 5,
                              savePredictions = TRUE,
                              classProbs = TRUE)

## create a vector of base learners
base_learners <- c('rpart', 'knn', 'svmRadial')

## create and summarize the list of base learners
all_models <- caretList(wheat_var ~ .,
                        data = training,
                        trControl = train_control,
                        methodList = base_learners)
summary(all_models)
```
-->

